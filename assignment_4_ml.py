# -*- coding: utf-8 -*-
"""ASSIGNMENT 4 ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iBEm4WZwM70lWTG7Faa1umnfQHNRtqAi
"""



"""**QUESTION 1:-**"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Generate a dataset with 7 highly correlated features
np.random.seed(42)
n_samples = 1000
X = np.random.rand(n_samples, 7)
# Creating high correlation
X[:, 1] = X[:, 0] + np.random.normal(0, 0.01, n_samples)
X[:, 2] = X[:, 0] + np.random.normal(0, 0.01, n_samples)
X[:, 3] = X[:, 0] + np.random.normal(0, 0.01, n_samples)
X[:, 4] = X[:, 0] + np.random.normal(0, 0.01, n_samples)
X[:, 5] = X[:, 0] + np.random.normal(0, 0.01, n_samples)
X[:, 6] = X[:, 0] + np.random.normal(0, 0.01, n_samples)

# Target variable
y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.normal(0, 0.1, n_samples)

# Convert to DataFrame
data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(7)])
data['target'] = y

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)

"""**Ridge Regression using Gradient Descent**"""

class RidgeRegression:
    def __init__(self, learning_rate=0.01, regularization_param=0.1, n_iterations=1000):
        self.learning_rate = learning_rate
        self.regularization_param = regularization_param
        self.n_iterations = n_iterations
        self.weights = None

    def fit(self, X, y):
        m, n = X.shape
        self.weights = np.zeros(n)

        for _ in range(self.n_iterations):
            predictions = X.dot(self.weights)
            errors = predictions - y
            gradient = (2/m) * (X.T.dot(errors) + self.regularization_param * self.weights)
            self.weights -= self.learning_rate * gradient

    def predict(self, X):
        return X.dot(self.weights)

    def cost_function(self, X, y):
        m = len(y)
        predictions = X.dot(self.weights)
        return (1/m) * np.sum((predictions - y)**2) + self.regularization_param * np.sum(self.weights**2)

best_cost = float('inf')
best_params = {}
learning_rates = [0.0001, 0.001, 0.01, 0.1, 1]
regularization_params = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]

for lr in learning_rates:
    for reg in regularization_params:
        model = RidgeRegression(learning_rate=lr, regularization_param=reg)
        model.fit(X_train.values, y_train.values)
        cost = model.cost_function(X_train.values, y_train.values)
        if cost < best_cost:
            best_cost = cost
            best_params = {'learning_rate': lr, 'regularization_param': reg}

print("Best parameters:", best_params)



"""**QUESTION 2 :-**"""

import pandas as pd

# Load the dataset
hitters = pd.read_csv('Hitters.csv')

# Fill missing values in the 'Salary' column
hitters['Salary'].fillna(hitters['Salary'].mean(), inplace=True)

# Convert categorical columns to dummy variables
hitters = pd.get_dummies(hitters, drop_first=True)

X = hitters.drop('Salary', axis=1)  # Features
y = hitters['Salary']                # Target variable

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # Fit and transform on training data
X_test = scaler.transform(X_test)        # Only transform on testing data

from sklearn.linear_model import LinearRegression, Ridge, Lasso

# Create model instances
linear_model = LinearRegression()
ridge_model = Ridge(alpha=0.5748)
lasso_model = Lasso(alpha=0.5748)

# Fit the models
linear_model.fit(X_train, y_train)
ridge_model.fit(X_train, y_train)
lasso_model.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error, r2_score

models = [linear_model, ridge_model, lasso_model]
for model in models:
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    print(f"{model.__class__.__name__}:")
    print(f"  RÂ² score: {r2:.4f}")
    print(f"  Mean Squared Error: {mse:.4f}")



"""**QUESTION 3**"""

import numpy as np
import pandas as pd
from sklearn.linear_model import RidgeCV, LassoCV

# Instead of load_boston, use fetch_california_housing
from sklearn.datasets import fetch_california_housing

# Load the California housing dataset
housing = fetch_california_housing()
X = housing.data
y = housing.target

alphas = np.logspace(-3, 3, 7)  # Alpha values from 0.001 to 1000

# Ridge Regression with Cross-Validation
ridge_cv = RidgeCV(alphas=alphas, cv=5)  # 5-fold cross-validation
ridge_cv.fit(X, y)

print(f"Best alpha for RidgeCV: {ridge_cv.alpha_}")

# Lasso Regression with Cross-Validation
lasso_cv = LassoCV(alphas=alphas, cv=5)  # 5-fold cross-validation
lasso_cv.fit(X, y)

print(f"Best alpha for LassoCV: {lasso_cv.alpha_}")